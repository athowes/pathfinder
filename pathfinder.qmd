---
title: "Pathfinder"
subtitle: "STF Lab Meeting"
author: Adam Howes
format: revealjs
bibliography: citations.bib
---

```{r}
library(ggplot2)
library(purrr)
pal <- c("#56B4E9", "#009E73", "#E69F00")
```

## What is Pathfinder

* A black box variational inference method [@zhang2022pathfinder]
  * Variational inference: approximates the posterior within a variational family of distributions
  * Black box: only uses the log-posterior

## Why are we interested in Pathfinder?

* We have probabilistic models that we would like to perform Bayesian inference for
  * Routinely fit 100s of models, and require reliable methods
* For complex models or large data, MCMC and HMC can be impractical (not scalable)
  * Even if we are to use MCMC and HMC, they require good initialisation values (burn-in)

## Example applications

* As to why I am interested in this
  * Estimating the effective reproductive number $R_t$ across US states each week
  * Estimating epidemiological delays with line list data
* Others are using Pathfinder for e.g. forecasting

## Features of pathfinder

* Claimed "state-of-the-art" method for
1. Black box VI
2. MCMC burn-in (I assume this means initialisation)
* Fast (-er than ADVI and HMC) and (almost embarrassingly) parallelisable
* Robust to "varying curvature, minor multimodality, avoiding variance and non-convergence of SGD"
* A little more accurate (than? I'd guess ADVI)

## Implementation

Focus here on implementation in Stan [@carpenter2017stan]

* Stan also has Laplace, ADVI, and HMC
* CFA related work using Stan includes `EpiNow2`, `epinowcast`, `epidist`

::: {.callout-note}
Also implemented in other packages (presumably).
Julia?
Python?
Others better placed to speak to this than me!
:::

## Stan set-up

* Defined unnormalised log posterior $\log p(\theta \, | \, y)$
* Continuous parameters $\theta$ transformed to $\mathbb{R}^N$
* Use automatic differentiation to compute $\nabla_\theta \log p(\theta \, | \, y)$
  * Stan's AD is optimised for CPUs rather than GPUs

## Black box VI

* VI is faster than MCMC because it swaps sampling for optimisation
* Choose approximating family e.g. $\mathcal{N}(\theta \, | \, \mu, \Sigma)$
* Find parameters which minimise divergence to posterior
$$
\mu^\star, \Sigma^\star = \arg \min_{\mu, \Sigma} \text{KL}[\mathcal{N}(\theta \, | \, \mu, \Sigma) || p(\theta \, | \, y)]
$$

* Different types of assumptions about $\Sigma$
  * Mean-field: $\Sigma = \text{diag}(\sigma_1, \ldots, \sigma_N)$ i.e. no correlation structure

## How to make this "Bayesian"?

* Generate draws from $\theta^{(m)} \sim \mathcal{N}(\theta \, | \, \mu, \Sigma)$ for $m = 1, \ldots, M$
* Use the draws to compute any relevant quantities (after transforming them back to constrained scale)
  * Mention of also doing importance sampling here
* For "machine learners" VI is typically used to generate $\hat \theta$

## Note on design for initialising MCMC

* Prefer to be concentrated within the target rather than outside support
  * HMC runs slower far outside the support: "stiff"
* Goal of pathfinder is to avoid this problem

::: {.callout-note}
How do goals of "inference" differ from goals of "initialisation"?
Coincidence (requiring justification) if they are exactly the same!
:::

## Pathfinder motivation (1)

```{r fig.cap = "Samples are usually far from the MAP (more so in high dimensions)!"}
df <- purrr::map_df(2^{0:8}, function(x) {
  samps <- sqrt(rchisq(n = 1000, df = x))
  quans <- quantile(samps, c(0.025, 0.5, 0.975))
  return(c("dof" = x, quans))
})

ggplot(df, aes(x = dof, y = `50%`)) +
  geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`), fill = "#FFE7B1") +
  geom_line(color = pal[3]) +
  theme_minimal() +
  labs(x = "Dimension", y = "Euclidean distance from mode of sample from MVN")
```
##

```{r}
library(ggplot2)
dim <- 2

lp_histogram <- function(dim) {
  samps <- MASS::mvrnorm(n = 1000, mu = rep(0, times = dim), Sigma = diag(nrow = dim, ncol = dim))
  lp <- apply(dnorm(samps, log = TRUE), 1, sum)
  text <- paste0("Draws from a MVN in ", dim, " dimensions") 
  ggplot(data = data.frame("lp" = lp) , aes(x = lp, after_stat(density))) +
    geom_histogram(fill = pal[2], alpha = 0.7) +
    theme_minimal() +
    labs(x = "Log probability", y = "Density", title = text)
}

lp_histogram(4)
```

##

```{r}
lp_histogram(16)
```

## Pathfinder motivation (2)

* Want to take a draw (and or initialise) at around expected log density
* Intermediate value theorem: iterative optimisation algorithm path passes from the tail, through the typical set, and to the mode
* Find a way to do the optimisation, and try to figure out if we're in the typical set at each point

## First try from Bob

* Try to run MCMC chains along each point along optimisation trajectory
  * If log density is increasing $\implies$ in the tail
  * If log density is going up and down $\implies$ in the typical set
  * If log density is decreasing $\implies$ near the mode
* Don't understand what this is aiming to do exactly but suggested it didn't work due to all the issues with MCMC

## Second try from Lu

* Estimate a hyper-elliptical surface containing 90% of the probability mass
  * Get volume using Laplace approximation
  * Choose points whose density times approximate volume is high
* I think in 2D this would be a band. As you move to higher dimensions it'd need to be less wide a band (?)

## Third try from Lu and Aki (Pathfinder!)

* Run optimiser first (L-BFGS, quasi-Newton)
* Gaussian approximation at each point on the trajectory (using calculations from L-BFGS)
* Sample from the Gaussian approximation with the lowest KL divergence to the posterior 

##

![This is a simple example that you don't really need Pathfinder for. A Laplace approximation would do.](fig1.png)

##

![For the funnel example it becomes clearer why Pathfinder might be a good idea. Iteration 6 is better than iteration 13, say. Definitely as an initialisation.](fig2.png)

##

![When is the right time to use approximate methods, and when is the right time to redesign the model?](converge.png)

## Some of my thoughts

* Sometimes it could just be that computation is expensive but the posterior is simple
  * Tall data
* Correlated posteriors sample slowly
  * But perhaps likely to be poorly estimated by VI and Laplace methods too...

## Comparison for `epidist`

* I've been working on a vignette for `epidist` demonstrating Laplace, ADVI, Pathfinder, and HMC

## Bibliography