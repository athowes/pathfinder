---
title: "Pathfinder"
subtitle: "STF Lab Meeting"
author: Adam Howes
format: revealjs
bibliography: citations.bib
---

```{r}
library(ggplot2)
library(purrr)
pal <- c("#56B4E9", "#009E73", "#E69F00")
```

## What is Pathfinder

* A black box variational inference method [@zhang2022pathfinder]
  * Variational inference: approximates the posterior within a variational family of distributions
  * Black box: only uses the log-posterior

## Why are we interested in Pathfinder?

* We have probabilistic models that we would like to perform Bayesian inference for
  * Routinely fit 100s of models, and require reliable methods
* For complex models or large data, MCMC and HMC can be impractical (not scalable)
  * Even if we are to use MCMC and HMC, they require good initialisation values (burn-in)

## My interest

* Both to improve initialisation and to act as drop in replacement
* Estimating the effective reproductive number $R_t$ across US states each week
  * NNH team has some $R_t$ fits which display convergence problems
  * Interested in extensions of the model to be more complex and use more data
* Estimating epidemiological delays with line list data

## Features of pathfinder

* Claim to be "state-of-the-art" for
1. Black box VI
2. MCMC burn-in (I assume this means initialisation)
* Faster than ADVI and HMC (phase I) and (almost embarrassingly) parallelisable
* Robust to "varying curvature, minor multimodality, avoiding variance and non-convergence of SGD"
* A little more accurate (than ADVI and HMC [phase I])

## Implementation

Focus here on implementation in Stan [@carpenter2017stan]

* Stan also has Laplace, ADVI, and HMC
* CFA related work using Stan includes `EpiNow2`, `epinowcast`, `epidist`

::: {.callout-note}
Also implemented in other packages (presumably).
Julia?
Python?
Others better placed to speak to this than me!
:::

## Stan set-up

* Defined unnormalised log posterior $\log p(\theta \, | \, y)$
* Continuous parameters $\theta$ transformed to $\mathbb{R}^N$
* Use automatic differentiation to compute $\nabla_\theta \log p(\theta \, | \, y)$
  * Stan's AD is optimised for CPUs rather than GPUs

## Hamiltonian Monte Carlo

* HMC (usual Stan MCMC algorithm) has hyperparameters like discretisation time $\epsilon$, metric $M$ and steps taken $L$ tuned during warm-up
* Warm-up has three phases
  * Phase I `init_buffer = 75`: Aims to "find the typical set". Tune the step-size
  * Phase II `window = 50` (doubling): Tune the metric
  * Phase III `term_buffer = 25`: Re-tune the step-size (given metric)
* And then afterwards there is the sampling phase

##

![Warm-up phases from the Stan manual.](phases.png)
  
## Black box VI

* VI is faster than MCMC because it swaps sampling for optimisation
* Choose approximating family e.g. $\mathcal{N}(\theta \, | \, \mu, \Sigma)$
* Find parameters which minimise divergence to posterior
$$
\mu^\star, \Sigma^\star = \arg \min_{\mu, \Sigma} \text{KL}[\mathcal{N}(\theta \, | \, \mu, \Sigma) || p(\theta \, | \, y)]
$$

* Different types of assumptions about $\Sigma$
  * Mean-field: $\Sigma = \text{diag}(\sigma_1, \ldots, \sigma_N)$ i.e. no correlation structure

## How to make this "Bayesian"?

* Generate draws from $\theta^{(m)} \sim \mathcal{N}(\theta \, | \, \mu, \Sigma)$ for $m = 1, \ldots, M$
* Use the draws to compute any relevant quantities (after transforming them back to constrained scale)
  * Mention of also doing importance sampling here
* For "machine learners" VI is typically used to generate $\hat \theta$

## Note on design for initialising MCMC

![This illustrates potential issue with e.g. mean-field VI. For initialisation, we prefer to be concentrated within the target rather than outside support.](bishop.png)

::: {.callout-note}
How do goals of "inference" differ from goals of "initialisation"?
Coincidence (requiring justification) if they are exactly the same!
:::

## Pathfinder motivation (1)

```{r fig.cap = "Samples are usually far from the MAP (more so in high dimensions)!"}
df <- purrr::map_df(2^{0:8}, function(x) {
  samps <- sqrt(rchisq(n = 1000, df = x))
  quans <- quantile(samps, c(0.025, 0.5, 0.975))
  return(c("dof" = x, quans))
})

ggplot(df, aes(x = dof, y = `50%`)) +
  geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`), fill = "#FFE7B1") +
  geom_line(color = pal[3]) +
  theme_minimal() +
  labs(x = "Dimension", y = "Euclidean distance from mode of sample from MVN")
```
##

```{r}
lp_histogram <- function(dim) {
  samps <- MASS::mvrnorm(n = 1000, mu = rep(0, times = dim), Sigma = diag(nrow = dim, ncol = dim))
  lp <- apply(dnorm(samps, log = TRUE), 1, sum)
  text <- paste0("Draws from a MVN in ", dim, " dimensions") 
  ggplot(data = data.frame("lp" = lp) , aes(x = lp, after_stat(density))) +
    geom_histogram(fill = pal[2], alpha = 0.7) +
    theme_minimal() +
    labs(x = "Log probability", y = "Density", title = text)
}

lp_histogram(4)
```

##

```{r}
lp_histogram(16)
```

## Pathfinder motivation (2)

* Want to take a draw (and or initialise) at around expected log density
* Intermediate value theorem: iterative optimisation algorithm path passes from the tail, through the typical set, and to the mode
* Find a way to do the optimisation, and try to figure out if we're in the typical set at each point

## First try from Bob

* Try to run MCMC chains along each point along optimisation trajectory
  * If log density is increasing $\implies$ in the tail
  * If log density is going up and down $\implies$ in the typical set
  * If log density is decreasing $\implies$ near the mode
* Don't understand what this is aiming to do exactly but suggested it didn't work due to all the issues with MCMC

## Second try from Lu

* Estimate a hyper-elliptical surface containing 90% of the probability mass
  * Get volume using Laplace approximation
  * Choose points whose density times approximate volume is high
* I think in 2D this would be a band. As you move to higher dimensions it'd need to be less wide a band (?)

## Third try from Lu and Aki (Pathfinder!)

* Run optimiser first (L-BFGS, quasi-Newton)
  * (quasi means that the Hessian is not directly available)
* Gaussian approximation at each point on the trajectory (using calculations from L-BFGS)
* Sample from the Gaussian approximation with the lowest KL divergence to the posterior 

##

![This is a simple example that you don't really need Pathfinder for. A Laplace approximation would do.](fig1.png)

##

![For the funnel example it becomes clearer why Pathfinder might be a good idea. Iteration 6 is better than iteration 13, say. Definitely as an initialisation.](fig2.png)

## Some notes

* After running optimisation, computation at each point can be done in parallel
  * KL-divergence evaluated using Monte Carlo
* Pathfinder is like early-stopping VI but with 1) automated stopping 2) additional entropy and mode avoidance
* Uses covariances from L-BFGS row-rank plus diagonal inverse Hessian approximation -- so in some sense I think these are precomputed as a part of the optimisation algorithm

## Evaluation using `posteriordb`

* Around 50 models in `posteriordb` with 10,000 reference draws (thinned HMC) [@Magnusson_posteriordb_a_set_2023]
  * Not carefully parameterised for HMC

##

![Uses 1-2 orders of magnitude less log density evaluations than other methods. Similar story for gradients. "This is the bottleneck". What about for other parts of the algorithm e.g. KL evaluations, sample generation? "It's embarrasingly parallel"](fig5a.png)

## Evaluating if it works

* Measure distance from approximate posterior draws to gold-standard reference draws
* Wanted to avoid using KL-divergence for evaluation (as this is in the design of the algorithm)
  * Use Wasserstein-1 distance from optimal transport

##

![](fig3.png)

## Multi-pathfinder

* Issues
  * L-BFGS getting stuck
  * Posterior not being Gaussian
* Partial solution run Pathfinder >1 times (multi-Pathfinder)
  * Now you have samples from an (equal weighted?) mixture of Gaussians (with low-rank plus diagonal covariance)
  * Importance resample the draws with Pareto smoothing (PSIS I imagine) (perhaps always do this, even for single Pathfinder?)

##

!["Mostly a bit better but sometimes much better."](fig4.png)

## Summary of why Pathfinder works

* quasi-Newton optimisation on the log-posterior is faster and more stable than SGD on the ELBO (what ADVI does)
* low-rank plus diagonal covariance from L-BFGS is better than diagonal but not as costly or difficult to estimate as full rank
* Evaluation of the ELBO is parallel
* Multi-Pathfinder has a mixture of Gaussians, which is better than a single one
* Minor nodes can be pruned and draws polished with PSIS

##

![When is the right time to use approximate methods, and when is the right time to redesign the model?](converge.png)

## Some of my thoughts

* Sometimes it could just be that computation is expensive but the posterior is simple e.g. big data
* Correlated posteriors sample slowly e.g. complex models
  * Perhaps likely to be poorly estimated by simple VI and Laplace methods
  * Hope for some intermediate here? e.g. multi-Pathfinder
* Maybe practically possible to parameterise all of our models as expert HMC users? Or for all of our packaged models to have users generate expert HMC parameterisations
  * In this case would we prefer to have forgiving algorithms (e.g. avoid funnel problem)
  * How robust to posterior misspecification to be? Any problem should be fixed at model level? What if OK to fix at inference level?

## Comparison for `epidist`

* I've been working on a vignette for `epidist` demonstrating Laplace, ADVI, Pathfinder, and HMC
* If time, I'll screenshare where I am with this

## Bibliography